{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Assignment2.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/michalis0/DataMining_and_MachineLearning/blob/master/Assignment2/Assignment2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9GDHMpV5QHV"
      },
      "source": [
        "# Classification\n",
        "\n",
        "## 1: Logistic Regression "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMXSZG845QHW"
      },
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import collections  as mc\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "import pandas as pd \n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "sns.set_style(\"white\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNcoqIdD5QHe"
      },
      "source": [
        "np.random.seed = 72"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnnBF0535QHj"
      },
      "source": [
        "### Load data\n",
        "\n",
        "For the first part we use the _income classification_ dataset from [kaggle](https://www.kaggle.com/lodetomasi1995/income-classification). This dataset contains some information about individual people and whether they earn more than $50K or not. Some of the features in the dataset are:\n",
        "\n",
        "> 1. `age` : age of the person.\n",
        "> 2. `workclass`: for which sector does the person work for, e.g, private sector, state\n",
        "> 3. `education`: the last degree the person has received.\n",
        "> 4. `occupation`: type of the job, e.g, services, sales, armed forces, etc.\n",
        "> 5. `race`\n",
        "> 6. `sex`\n",
        "> 7. `native-country`\n",
        "\n",
        "Your task is to predict whether a person earns more than $50K per year or not, i.e, you should use the `income` column as the labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xktAXDA5QHj"
      },
      "source": [
        "df = pd.read_csv(\"https://raw.githubusercontent.com/michalis0/DataMining_and_MachineLearning/master/Assignment2/data/incom_classification.csv\")\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HiNwZFwk5QHt"
      },
      "source": [
        "How many rows and columns does this dataset have?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WhuzicK5QHu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maZbKf-T5QHz"
      },
      "source": [
        "### base rate\n",
        "What is the base rate?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O13jTMeL5QHz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i69xlhpc5QH2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XuO4a-xV5QH9"
      },
      "source": [
        "### Important! \n",
        "\n",
        "__For all the questions below, fix the seed of random generators to 72.__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rviChsv5QH9"
      },
      "source": [
        "### Training\n",
        "\n",
        "Train a logistic regression model on this data-set. Use all of the features in the dataset. For the categorical features, encode `education`, `occupation`, and `native-country` with label encoding and the rest with one hot encoding. Split the dataset into 80% training and 20% test set.\n",
        "\n",
        "- what is the train accuracy?\n",
        "\n",
        "- what is the test accuracy?\n",
        "\n",
        "- what is the precision and recall for predicting the income class <=50K?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJ9hroA55QH-"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegressionCV, LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn import datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2VMZlXp5QIP"
      },
      "source": [
        "#### Numerical columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3IblnYL5QIQ"
      },
      "source": [
        "# extracting numerial attributes\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnlFjzoE5QIS"
      },
      "source": [
        "#### Categorical columns\n",
        "\n",
        "Encode the columns `workclass`, `race`, and `sex` using one-hot encoding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IF2twuOM5QIS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKaVIMYk5QIU"
      },
      "source": [
        "Encode the columns `education`, `occupation`, and `native-country` using label encoding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "noLZvi2U5QIV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9h_2e8U75QIX"
      },
      "source": [
        "Also encode the labels vector `y` to have \\[0, 1\\] labels instead of \\[<=50K, >50K\\]."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IgsNVbY-5QIa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQFHPEX05QId"
      },
      "source": [
        "Now concatonate all these features (numerical, label encoded, and one-hot encoded) into a single dataframe. You can use `pd.concat` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8tGQ4tk5QIe"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S39iHpz75QIh"
      },
      "source": [
        "#### Train/test splitting\n",
        "Now split the data into 80% training and 20% test set. Remember to set the random seed to 72."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3SLXxvt5QIi"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kQwySIQ5QIv"
      },
      "source": [
        "#### Standardization\n",
        "Standardize only the numerical features (__not the categorical features__) to have mean zero and standard deviation equal to 1. You can use sklearn `StandardScaler` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsfrbxNS5QIw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxLHmnli5QI1"
      },
      "source": [
        "#### Training\n",
        "Finally, train a Logistic Regression model on the processed dataset you just created. Use the following attributes for Logistic Regression.\n",
        "\n",
        "```\n",
        "LogisticRegressionCV(solver='lbfgs', cv=5, max_iter=1000, random_state=72)\n",
        "```\n",
        "\n",
        "The module `LogisticRegressionCV` enabels you to train a Logistic Regression model with cross validation. That is, it uses a logistic regression model with L2 regularizer and finds the coefficient of the regularizer (which is the hyper-parameter of the model) by doing cross validation. The attribute `cv` determines how many folds it uses for cross validation. By default it searches for the hyper-parameter in a list of 10 numbers between $10^{-4}$ and $10^4$ (in a logarithmic scale). As you know, using a regularized model improves the generalization ability of your model, in other words, it improves the test accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKGIH0vC5QI3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nEBzNEO5QJF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Y761ydh5QJH"
      },
      "source": [
        "#### Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8TqldCub5QJI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahCVhvBM5QJM"
      },
      "source": [
        "Now compute the confusion matrix of your classifer for the test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zO7lWy245QJN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "he40Y9ry5QJQ"
      },
      "source": [
        "## 2: Text Analytics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccDnT3K05QJR"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
        "from sklearn.base import TransformerMixin\n",
        "from sklearn.pipeline import Pipeline\n",
        "import spacy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDie6R5k5QJU"
      },
      "source": [
        "!python -m spacy download en"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l414DA1y5QJW"
      },
      "source": [
        "#### Load the data\n",
        "\n",
        "For this part we use the _twitter climate change sentiment dataset_ from [kaggle](https://www.kaggle.com/edqian/twitter-climate-change-sentiment-dataset). The dataset contains tweets related to the climate change topic. Each tweet is labeled as one of the following 4 classes:\n",
        "\n",
        "- `2`(News): the tweet links to factual news about climate change\n",
        "\n",
        "- `1`(Pro): the tweet supports the belief of man-made climate change\n",
        "\n",
        "- `0`(Neutral): the tweet neither supports nor refutes the belief of man-made climate change\n",
        "\n",
        "- `-1`(Anti): the tweet does not believe in man-made climate change\n",
        "\n",
        "\n",
        "Your task is to predict the sentiment of these tweets using the text analytics techniques you have learned in the lab and a logistic regression model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-k7XW4L5QJY"
      },
      "source": [
        "df = pd.read_csv(\"https://raw.githubusercontent.com/michalis0/DataMining_and_MachineLearning/master/Assignment2/data/twitter_sentiment_data.csv\")\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ih32d0x5QJd"
      },
      "source": [
        "#### base rate\n",
        "What is the base rate for this problem?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufoOHnB05QJd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJfpZO7Y5QJg"
      },
      "source": [
        "#### processing the tweets\n",
        "\n",
        "preprocess the tweets:\n",
        "- Remove the stopwords. Use the stop words from `spacy` package.\n",
        "\n",
        "- Remove the punctuation marks. Use the punctuation marks from the `string` package.\n",
        "\n",
        "- Lowercase all of the words.\n",
        "\n",
        "- Lemmatize all of the words. Lemmatize the words using the `spacy` package, similar to what you did in the lab!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-OGHF9dl5QJh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AETv_7NX5QJl"
      },
      "source": [
        "#### Train/test splitting\n",
        "Split the dataset into 80% training and 20% test set. Remeber to set the random seed to be 72."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKXXlozn5QJl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1CSBHAf5QJq"
      },
      "source": [
        "#### TF-IDF feature vectors\n",
        "\n",
        "create the TF-IDF feature vectors for the processed tweetes. These will construct you data features that you will use to train a classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFbS_TJ_5QJq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRBkPRru5QJw"
      },
      "source": [
        "#### Training\n",
        "\n",
        "Now train a logistic regression classifier on the TF-IDF vectors. Use the `LogisticRegression` module (without regularizer) from sklearn with the following attributes:\n",
        "\n",
        "```\n",
        "LogisticRegression(solver=\"lbfgs\", max_iter=1000, random_state=72)\n",
        "```\n",
        "\n",
        "We encourage you to make a pipeline that first vectorize the input text and then applies the classifier on the TF-IDF vectors. To do this you can use `Pipeline` from `sklearn.pipeline`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkwzDE965QJx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8I_rzMz75QJ0"
      },
      "source": [
        "#### Accuracy\n",
        "\n",
        "- What is the test accuracy of the classifier?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRJifzue5QJ0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWa8mYqR5QJ7"
      },
      "source": [
        "Compute the confusion matrix for this classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDjuRjLH5QJ7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7yEEvJv5QKC"
      },
      "source": [
        "#### Improving your classifier!\n",
        "\n",
        "What could you do more to improve the test accuracy of your classifier? Here's some suggestions:\n",
        "\n",
        "- Use regularized logistic regression and tune the hyper-parameter with cross-validation. This is similar to the classifier you used for the first part of this assignment (the income classifcation problem).\n",
        "\n",
        "- Apply further text preprocessing, e.g, removing the retweets in the form of RT @<user>, removing hashtags, removing duplicate tweets (if any), etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qggr-amq5QKC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}