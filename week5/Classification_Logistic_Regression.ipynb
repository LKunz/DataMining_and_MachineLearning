{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "colab": {
      "name": "Classification_Logistic_Regression.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/michalis0/DataMining_and_MachineLearning/blob/master/week5/Classification_Logistic_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zi22-gXYJoXW"
      },
      "source": [
        "# Data Mining and Machine Learning - Week 5\n",
        "# Classification\n",
        "\n",
        "Classification is part of **supervised learning**. The objective is to correctly assign objects to different, predifined categories or labels. An easy to understand example is classifying emails as “spam” or “not spam.”\n",
        "\n",
        "### Table of Contents\n",
        "#### 0. Summary of some important concepts\n",
        "#### 1. Basic Example\n",
        "* 1.1 Create Data\n",
        "* 1.2 Encoding of `Group`\n",
        "* 1.3 Plot `x1` and `x2` according to `target`\n",
        "* 1.4 Logistic Regression and Decision Boundary\n",
        "\n",
        "#### 2. Predict Ad Click\n",
        "* 2.1 Load and explore the dataset\n",
        "* 2.2 Exploratory Data Analysis\n",
        "* 2.3 Logistic Regression\n",
        "* 2.4 Logistic Regression with standardization\n",
        "\n",
        "#### 3. Multi Class Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XwvNGjJg6RB"
      },
      "source": [
        "## 0. Summary of some important concepts\n",
        "\n",
        "Suppose you have a sample of 100 people, 20 of whom have purchased a certain product. You want to predict whether a person will `buy` (1) or `not buy` (0) the product based on her characteristics (e.g. male/female, age, etc.).\n",
        "\n",
        "### A. Base rate\n",
        "Represents the degree of accuracy you would obtain without using an algorithm (i.e. the prior probability of the most common class). In the example, p(`buy`) = 0.2 and p(`not buy`) = 0.8. This means that you would obtain an accuracy of 80% without using an algorithm if you classify each person as `not buy`. Clearly, our approach should outperform the base rate.\n",
        "\n",
        "### B. Accuracy\n",
        "This is the number of correct decisions for a model out of the total number of decisions. This should be greater than the base rate.\n",
        "\n",
        "### C. True positive, true negative, false positive, false negative\n",
        "* True positive: True label is positive (`buy`) and algorithm classifies as positive (`buy`).\n",
        "* True negative: True label is negative (`not buy`) and algorithm classifies as negative (`not buy`).\n",
        "* False positive: True label is negative (`not buy`), but algorithm classifies as positive (`buy`).\n",
        "* False negative: True label is positive (`buy`), but algorithm classifies as negative (`not buy`).\n",
        "\n",
        "We can summarize these concepts using the confusion matrix.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5f3DoMWJoXY"
      },
      "source": [
        "# Import required packages\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import datasets\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.utils.multiclass import unique_labels\n",
        "\n",
        "%matplotlib inline\n",
        "sns.set_style(\"dark\")"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4_fSk_KJoXc"
      },
      "source": [
        "## 1. Basic Example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNo4lszWJoXd"
      },
      "source": [
        "### 1.1 Create Data\n",
        "For this example, we create data from scratch. We have a categorical variable (`Group`), two numerical variables (`x1` and `x2`) and the `target` is a binary variable. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WzKIjiUmJoXf"
      },
      "source": [
        "sample = [\n",
        "    [\"A\", 0.1, 0.2, 0],\n",
        "    [\"A\", 0.3, 0.05, 0],\n",
        "    [\"B\", 0.3, 0.2, 0],\n",
        "    [\"D\", 0.7, 0.65, 1],\n",
        "    [\"B\", 0.25, 0.3, 0],\n",
        "    [\"A\", 0.85, 0.55, 1],\n",
        "    [\"C\", 0.1, 0.45, 0],\n",
        "    [\"C\", 0.9, 0.85, 1],\n",
        "    [\"D\", 0.95, 0.55, 1],\n",
        "    [\"B\", 0.8, 0.8, 1]\n",
        "]\n",
        "\n",
        "df = pd.DataFrame(sample, columns=[\"Group\", \"x1\", \"x2\", \"target\"])\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_r_wF21JoXm"
      },
      "source": [
        "### 1.2 Encoding of `Group`\n",
        "\n",
        "We illustrate how to encode categorical features using:\n",
        "* **One-hot encoding**: create a dummy variable for each category.\n",
        "* **Label encoding**: assign integers to the different categories. Useful for ordered data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewChFj85JoXm"
      },
      "source": [
        "# One-hot encoding\n",
        "one_hot = OneHotEncoder()\n",
        "cat_to_onehot = one_hot.fit_transform(df[[\"Group\"]]).toarray()\n",
        "cat_to_onehot = pd.DataFrame(cat_to_onehot, columns=one_hot.categories_)\n",
        "cat_to_onehot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O66GBDLWJoXr"
      },
      "source": [
        "# Label encoding\n",
        "le = LabelEncoder()\n",
        "cat_to_label = pd.Series(le.fit_transform(df[\"Group\"]))\n",
        "cat_to_label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yaQ9krO3JoXv"
      },
      "source": [
        "le.classes_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDZO13NmJoXz"
      },
      "source": [
        "### 1.3 Plot `x1` and `x2` according to `target`\n",
        "We are now interested in predicting `target` based on `x1` and `x2`. We first generate a plot."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "704eSOsvpG1v"
      },
      "source": [
        "df.plot.scatter(\"x1\", \"x2\", c=\"target\", colormap=\"coolwarm_r\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSG5lj13kUKP"
      },
      "source": [
        "We can see a clear separation. Points with low value of `x1` and `x2` are in the first class (`target` = 0) and points with high value of `x1` and `x2` are in the second class (`target` = 1). We can further compute the base rate (i.e the probability of the most common class)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9PSDCSmk-2u"
      },
      "source": [
        "# Base rate\n",
        "max(len(df[df[\"target\"] == 0]) / len(df), len(df[df[\"target\"] == 1]) / len(df))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvNdCm7LJoX6"
      },
      "source": [
        "### 1.4 Logistic Regression and Decision Boundary\n",
        "We now fit a logistic regression and generate a plot showing the decision boundary of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3dPadKaJoX7"
      },
      "source": [
        "# Fit model\n",
        "X = df[[\"x1\", \"x2\"]].values\n",
        "y = df[\"target\"]\n",
        "LR = LogisticRegressionCV()\n",
        "LR.fit(X, y)\n",
        "\n",
        "# Plot\n",
        "x_min, x_max = X[:, 0].min() - .1, X[:, 0].max() + .1\n",
        "y_min, y_max = X[:, 1].min() - .1, X[:, 1].max() + .1\n",
        "h = .005  # step size in the mesh\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
        "Z = LR.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "\n",
        "# Put the result into a color plot\n",
        "Z = Z.reshape(xx.shape)\n",
        "plt.figure(1, figsize=(15, 10))\n",
        "plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)\n",
        "\n",
        "# Plot also the training points and two new points (p1 and p2)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', cmap=plt.cm.Paired)\n",
        "plt.scatter(0.4, 0.4, c='black') # p1\n",
        "plt.scatter(0.5, 0.5, c='black') # p2\n",
        "plt.text(0.37, 0.37, 'p1')\n",
        "plt.text(0.51, 0.5, 'p2')\n",
        "plt.xlabel('x1', fontsize=15)\n",
        "plt.ylabel('x2', fontsize=15)\n",
        "\n",
        "plt.xlim(xx.min(), xx.max())\n",
        "plt.ylim(yy.min(), yy.max())\n",
        "plt.xticks(())\n",
        "plt.yticks(())\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osHCamD9JoX_"
      },
      "source": [
        "# Prediction for p1 and p2\n",
        "p = pd.DataFrame([[0.4, 0.4], \n",
        "                 [0.5, 0.5]], columns=[\"x1\", \"x2\"])\n",
        "LR.predict(p)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLSBZ0lCl4Lf"
      },
      "source": [
        "# Score of the model --> better than base rate :)\n",
        "LR.score(X, y)\n",
        "# accuracy_score(LR.predict(X), y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCGZIvfxJoYE"
      },
      "source": [
        "## 2. Predict Ad Click\n",
        "In this section, we use **Logistic Regression** to predict whether or not a particular Internet user will click on an advertisement. You can find the data set [here](https://www.kaggle.com/fayomi/advertising)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzYt2MpxJoYF"
      },
      "source": [
        "### 2.1 Load and explore the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ZyIGSeLJoYF"
      },
      "source": [
        "data = pd.read_csv(\"https://raw.githubusercontent.com/michalis0/DataMining_and_MachineLearning/master/week5/data/advertising.csv\")\n",
        "data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImQciq5utXNJ"
      },
      "source": [
        "The data set has 1000 rows and 10 features:\n",
        "\n",
        "* `Daily Time Spent on Site`: consumer time on site in minutes\n",
        "* `Age`: cutomer age in years\n",
        "* `Area Income`: Avg. Income of geographical area of consumer\n",
        "* `Daily Internet Usage`: Avg. minutes a day consumer is on the internet\n",
        "* `Ad Topic Line`: Headline of the advertisement\n",
        "* `City`: City of consumer\n",
        "* `Male`: Whether or not consumer was male\n",
        "* `Country`: Country of consumer\n",
        "* `Timestamp`: Time at which consumer clicked on Ad or closed window\n",
        "* `Clicked on Ad`: 0 or 1 indicated clicking on Ad"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkjBtInaJoYJ"
      },
      "source": [
        "data.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDS7MgQaJoYM"
      },
      "source": [
        "# Date format\n",
        "data[\"Timestamp\"] = pd.to_datetime(data[\"Timestamp\"], format=\"%Y-%m-%d\")"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVukaoI5JoYQ"
      },
      "source": [
        "data.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ml8LBetWJoYW"
      },
      "source": [
        "data.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kywiPIvpJoYb"
      },
      "source": [
        "### 2.2 Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vEfRQywJoYd"
      },
      "source": [
        "# Age repartition\n",
        "plt.figure(figsize=(10, 8))\n",
        "data.Age.hist(bins=data.Age.nunique())\n",
        "plt.xlabel('Age')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEdqMUixJoYg"
      },
      "source": [
        "# Time on site\n",
        "plt.figure(figsize=(10, 8))\n",
        "data[\"Daily Time Spent on Site\"].hist()\n",
        "plt.xlabel(\"Daily Time Spent on Site\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUpDTfrRJoYj"
      },
      "source": [
        "# Does younger people spend more time on site?\n",
        "sns.jointplot(data[\"Daily Time Spent on Site\"], data.Age)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3Uw9W8TJoYn"
      },
      "source": [
        "sns.jointplot(data[\"Daily Time Spent on Site\"], data[\"Daily Internet Usage\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "auXGwDFVJoYq"
      },
      "source": [
        "sns.pairplot(data, hue='Clicked on Ad')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owtUHt8ZJoYt"
      },
      "source": [
        "data2 = data[[\"Daily Time Spent on Site\", \"Daily Internet Usage\", \"Clicked on Ad\"]]\n",
        "sns.pairplot(data2, hue=\"Clicked on Ad\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YfGaxXc0JoYv"
      },
      "source": [
        "### 2.3 Logistic Regression\n",
        "\n",
        "Logistic regression is the classic linear classification algorithm for two-class problems."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebzvK8hvNOMe"
      },
      "source": [
        "#### 2.3.1 Theory\n",
        "\n",
        "##### Logistic Regression\n",
        "\n",
        "Logistic regression is named for the function used at the core of the method, the [logistic function](https://en.wikipedia.org/wiki/Logistic_function).\n",
        "\n",
        "The logistic function, also called the **`Sigmoid function`** was developed by statisticians to describe properties of population growth in ecology, rising quickly and maxing out at the carrying capacity of the environment. It’s an S-shaped curve that can take any real-valued number and map it into a value between 0 and 1, but never exactly at those limits.\n",
        "\n",
        "$$\\frac{1}{1 + e^{-x}}$$\n",
        "\n",
        "$e$ is the base of the natural logarithms and $x$ is value that you want to transform via the logistic function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYO9o-tUJoYw"
      },
      "source": [
        "x = np.linspace(-6, 6, num=1000)\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(x, (1 / (1 + np.exp(-x))))\n",
        "plt.title(\"Sigmoid Function\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzBjBzhPJoYy"
      },
      "source": [
        "The logistic regression equation has a very similar representation like linear regression. The difference is that the output value being modelled is binary in nature.\n",
        "\n",
        "$$\\hat{y}=\\frac{e^{\\beta_0+\\beta_1x_1}}{1+\\beta_0+\\beta_1x_1}$$\n",
        "\n",
        "or\n",
        "\n",
        "$$\\hat{y}=\\frac{1.0}{1.0+e^{-\\beta_0-\\beta_1x_1}}$$\n",
        "\n",
        "$\\beta_0$ is the intecept term\n",
        "\n",
        "$\\beta_1$ is the coefficient for $x_1$\n",
        "\n",
        "$\\hat{y}$ is the predicted output with real value between 0 and 1. To convert this to binary output of 0 or 1, this would either need to be rounded to an integer value or a cutoff point be provided to specify the class segregation point.\n",
        "***\n",
        "##### Learning the Logistic Regression Model\n",
        "\n",
        "The coefficients (Beta values b) of the logistic regression algorithm must be estimated from your training data. This is done using [maximum-likelihood estimation](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation).\n",
        "\n",
        "Maximum-likelihood estimation is a common learning algorithm used by a variety of machine learning algorithms, although it does make assumptions about the distribution of your data (more on this when we talk about preparing your data).\n",
        "\n",
        "The best coefficients would result in a model that would predict a value very close to 1 (e.g. male) for the default class and a value very close to 0 (e.g. female) for the other class. The intuition for maximum-likelihood for logistic regression is that a search procedure seeks values for the coefficients (Beta values) that minimize the error in the probabilities predicted by the model to those in the data (e.g. probability of 1 if the data is the primary class).\n",
        "\n",
        "We are not going to go into the math of maximum likelihood. It is enough to say that a minimization algorithm is used to optimize the best values for the coefficients for your training data. This is often implemented in practice using efficient numerical optimization algorithm (like the Quasi-newton method).\n",
        "\n",
        "When you are learning logistic, you can implement it yourself from scratch using the much simpler gradient descent algorithm.\n",
        "\n",
        "##### Prepare Data for Logistic Regression\n",
        "The assumptions made by logistic regression about the distribution and relationships in your data are much the same as the assumptions made in linear regression.\n",
        "\n",
        "Much study has gone into defining these assumptions and precise probabilistic and statistical language is used. My advice is to use these as guidelines or rules of thumb and experiment with different data preparation schemes.\n",
        "\n",
        "Ultimately in predictive modeling machine learning projects you are laser focused on making accurate predictions rather than interpreting the results. As such, you can break some assumptions as long as the model is robust and performs well.\n",
        "\n",
        "- **Binary Output Variable:** This might be obvious as we have already mentioned it, but logistic regression is intended for binary (two-class) classification problems. It will predict the probability of an instance belonging to the default class, which can be snapped into a 0 or 1 classification.\n",
        "- **Remove Noise:** Logistic regression assumes no error in the output variable (y), consider removing outliers and possibly misclassified instances from your training data.\n",
        "- **Gaussian Distribution:** Logistic regression is a linear algorithm (with a non-linear transform on output). It does assume a linear relationship between the input variables with the output. Data transforms of your input variables that better expose this linear relationship can result in a more accurate model. For example, you can use log, root, Box-Cox and other univariate transforms to better expose this relationship.\n",
        "- **Remove Correlated Inputs:** Like linear regression, the model can overfit if you have multiple highly-correlated inputs. Consider calculating the pairwise correlations between all inputs and removing highly correlated inputs.\n",
        "- **Fail to Converge:** It is possible for the expected likelihood estimation process that learns the coefficients to fail to converge. This can happen if there are many highly correlated inputs in your data or the data is very sparse (e.g. lots of zeros in your input data)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9BR3u3oJoYz"
      },
      "source": [
        "#### 2.3.2 Implementing Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDyXeuVz0byL"
      },
      "source": [
        "# Base rate\n",
        "data[data[\"Clicked on Ad\"] == 0]\n",
        "#max(len(data[data[\"Clicked on Ad\"] == 0])/len(data), len(data[data[\"Clicked on Ad\"] == 1])/len(data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncTWvVdUunFA"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbCPd4rSJoYz"
      },
      "source": [
        "# Encode Country and City\n",
        "data[\"Country\"] = data.Country.astype('category').cat.codes\n",
        "data[\"City\"] = data.City.astype('category').cat.codes\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzU12-GOJoY3"
      },
      "source": [
        "# Select variables\n",
        "X = data.drop(['Timestamp', 'Clicked on Ad', 'Ad Topic Line'], axis=1)\n",
        "y = data['Clicked on Ad']\n",
        "X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1LZa1tRCuwfb"
      },
      "source": [
        "y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wqWJ5qmuwhn"
      },
      "source": [
        "# Train test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "X_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXPB_u5OuwUT"
      },
      "source": [
        "X_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vOgrS5FvB85"
      },
      "source": [
        "# Fit Model\n",
        "log_reg = LogisticRegressionCV(solver='lbfgs', cv=5, max_iter=1000)\n",
        "log_reg.fit(X_train, y_train)\n",
        "y_pred = log_reg.predict(X_test)\n",
        "y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nV6MDANvHaq"
      },
      "source": [
        "y_test.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROmK74dtJoY6"
      },
      "source": [
        "#### 2.3.3 Performance measurement"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fx3RfhBAJoY7"
      },
      "source": [
        "def evaluate(true, pred):\n",
        "    precision = precision_score(true, pred)\n",
        "    recall = recall_score(true, pred)\n",
        "    f1 = f1_score(true, pred)\n",
        "    print(f\"CONFUSION MATRIX:\\n{confusion_matrix(true, pred)}\")\n",
        "    print(f\"ACCURACY SCORE:\\n{accuracy_score(true, pred):.4f}\")\n",
        "    print(f\"CLASSIFICATION REPORT:\\n\\tPrecision: {precision:.4f}\\n\\tRecall: {recall:.4f}\\n\\tF1_Score: {f1:.4f}\")"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3eM4dTmNy5oK"
      },
      "source": [
        "evaluate(y_test, y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YgBrjdycyHFN"
      },
      "source": [
        "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
        "print(\"True positives: \" + str(tp))\n",
        "print(\"True negatives: \" + str(tn))\n",
        "print(\"False positives: \" + str(fp))\n",
        "print(\"False negatives: \" + str(fn))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DoRMMWxvveBH"
      },
      "source": [
        "###### Confusion matrix:\n",
        "* true positives (93): people who clicked on ad and were classified as clicked on ad.\n",
        "* true negatives (85): people who did not click on ad and were classified as did not click on ad.\n",
        "* false positives (4): people who did not click on as and were classified as clicked on as.\n",
        "* false negatives (18): people who clicked on ad and were classified as did not click on ad.\n",
        "\n",
        "###### Accuracy score:\n",
        "correct classifications / total = (85+93) / (85+93+18+4) = 0.89\n",
        "\n",
        "###### Precision:\n",
        "true positives / (true positives + false positives) = 93 / (93+4) = 0.9588\n",
        "\n",
        "###### Recall:\n",
        "true positives / (true positives + false negatives) = 93 / (93+18) = 0.8378\n",
        "\n",
        "###### F1 score:\n",
        "harmonic mean of precision and recall.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Roc3GhvQN8Bk"
      },
      "source": [
        "### 2.4 Logistic Regression with standardization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6KaSjespn7S"
      },
      "source": [
        "Standardization is helpful to give the same weight (or importance) to each predictor variable. The aim is to resize the s so that their mean equal 0 and their standard deviation equal 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pdaJee9OFFv"
      },
      "source": [
        "# Standardize features\n",
        "standardize = StandardScaler()\n",
        "standardize.fit(X_train, y_train)\n",
        "# !!!IMPORTANT: we must fit on the training set, not on the whole set!!!\n",
        "\n",
        "X_train_s = standardize.transform(X_train)\n",
        "X_test_s = standardize.transform(X_test)\n",
        "\n",
        "pd.DataFrame(X_train_s, columns=X_train.columns)#.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3flrTAKRp-x2"
      },
      "source": [
        "# Fit Logistic Regression and compute predictions\n",
        "log_reg_s = LogisticRegressionCV(solver='lbfgs', cv=5, max_iter=1000)\n",
        "log_reg_s.fit(X_train_s, y_train)\n",
        "y_pred_s = log_reg_s.predict(X_test_s)\n",
        "\n",
        "# Performance measurement\n",
        "evaluate(y_test, y_pred_s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efknHmAQJoZE"
      },
      "source": [
        "## 3. Multi Class Regression\n",
        "Let's now consider a classification problem with more than 2 target classes. For this we will use the iris data-set which has 3 target classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PENVlWMJoZF"
      },
      "source": [
        "iris = datasets.load_iris()\n",
        "X = iris.data[:, :2] # we only take the first two features\n",
        "y = iris.target\n",
        "X[:10] # first 10 instances"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBUaAknznkww"
      },
      "source": [
        "y # three classes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERBz7dhHJoZJ"
      },
      "source": [
        "# Train test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fIuOibAmJoZM"
      },
      "source": [
        "# Fit model\n",
        "log_reg = LogisticRegressionCV(solver='lbfgs', cv=5, max_iter=1000)\n",
        "log_reg.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0H7JpS6iJoZU"
      },
      "source": [
        "# Accuracy of training set\n",
        "log_reg.score(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlT0vM43n4aY"
      },
      "source": [
        "log_reg.score(X_test, y_test)\n",
        "# accuracy_score(log_reg.predict(X_test), y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvGK1Loroq6I"
      },
      "source": [
        "# True values\n",
        "y_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ftxUNiDpV9D"
      },
      "source": [
        "# Predictions\n",
        "log_reg.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O77K7nhwrumz"
      },
      "source": [
        "log_reg.predict_proba(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PU_4ah5xp6Y8"
      },
      "source": [
        "# Create DateFrame with probabilities, predictions, and true classes in test set\n",
        "iris_LR_summary = pd.DataFrame(np.round(log_reg.predict_proba(X_test), 2), columns=[\"p(0)\", \"p(1)\", \"p(2)\"])\n",
        "iris_LR_summary[\"Prediction\"] = log_reg.predict(X_test)\n",
        "iris_LR_summary[\"True Class\"] = y_test\n",
        "iris_LR_summary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8v1jl8fsh8e"
      },
      "source": [
        "# Confusion matrix - 4 errors out of 30 points\n",
        "confusion_matrix(y_test, log_reg.predict(X_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcmnRNzdsypn"
      },
      "source": [
        "def plot_confusion_matrix(y_true, y_pred, classes,\n",
        "                          normalize=False,\n",
        "                          title=None,\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    if not title:\n",
        "        if normalize:\n",
        "            title = 'Normalized confusion matrix'\n",
        "        else:\n",
        "            title = 'Confusion matrix, without normalization'\n",
        "\n",
        "    # Compute confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    # Only use the labels that appear in the data\n",
        "    classes = classes[unique_labels(y_true, y_pred)]\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "#     print(cm)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10,7))\n",
        "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    ax.figure.colorbar(im, ax=ax)\n",
        "    # We want to show all ticks...\n",
        "    ax.set(xticks=np.arange(cm.shape[1]),\n",
        "           yticks=np.arange(cm.shape[0]),\n",
        "           # ... and label them with the respective list entries\n",
        "           xticklabels=classes, yticklabels=classes,\n",
        "           title=title,\n",
        "           ylabel='True label',\n",
        "           xlabel='Predicted label')\n",
        "    plt.ylim([-0.5, 2.5])\n",
        "\n",
        "    # Rotate the tick labels and set their alignment.\n",
        "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
        "             rotation_mode=\"anchor\")\n",
        "\n",
        "    \n",
        "    # Loop over data dimensions and create text annotations.\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            ax.text(j, i, format(cm[i, j], fmt),\n",
        "                    ha=\"center\", va=\"center\",\n",
        "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "    fig.tight_layout();\n",
        "    return ax\n",
        "\n",
        "# Plot non-normalized confusion matrix\n",
        "plot_confusion_matrix(y_test, log_reg.predict(X_test), classes=iris.target_names,\n",
        "                      title='Confusion matrix, without normalization')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CW-_t6dQvFSc"
      },
      "source": [
        "# Plot the decision boundaries for test set. For that, we will assign a \n",
        "# color to each point in the mesh [x_min, x_max]x[y_min, y_max].\n",
        "x_min, x_max = X_test[:, 0].min() - .2, X_test[:, 0].max() + .2\n",
        "y_min, y_max = X_test[:, 1].min() - .2, X_test[:, 1].max() + .2\n",
        "h = .001  # step size in the mesh\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
        "Z = log_reg.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "\n",
        "# Put the result into a color plot\n",
        "Z = Z.reshape(xx.shape)\n",
        "plt.figure(1, figsize=(15, 10))\n",
        "plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)\n",
        "\n",
        "# Plot also the training points\n",
        "plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, edgecolors='k', cmap=plt.cm.Paired)\n",
        "plt.xlabel('Sepal length', fontsize=15)\n",
        "plt.ylabel('Sepal width', fontsize=15)\n",
        "\n",
        "plt.xlim(xx.min(), xx.max())\n",
        "plt.ylim(yy.min(), yy.max())\n",
        "plt.xticks(())\n",
        "plt.yticks(())\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzmaujuVteUz"
      },
      "source": [
        "# confusion matrix - training set\n",
        "\n",
        "# Plot non-normalized confusion matrix\n",
        "plot_confusion_matrix(y_train, log_reg.predict(X_train), classes=iris.target_names,\n",
        "                      title='Confusion matrix, without normalization')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "geLkykFHJoZi"
      },
      "source": [
        "# Plot the decision boundaries for training set. For that, we will assign a \n",
        "# color to each point in the mesh [x_min, x_max]x[y_min, y_max].\n",
        "x_min, x_max = X_train[:, 0].min() - .2, X_train[:, 0].max() + .2\n",
        "y_min, y_max = X_train[:, 1].min() - .2, X_train[:, 1].max() + .2\n",
        "h = .01  # step size in the mesh\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
        "Z = log_reg.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "\n",
        "# Put the result into a color plot\n",
        "Z = Z.reshape(xx.shape)\n",
        "plt.figure(1, figsize=(15, 10))\n",
        "plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)\n",
        "\n",
        "# Plot also the training points\n",
        "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, edgecolors='k', cmap=plt.cm.Paired)\n",
        "plt.xlabel('Sepal length', fontsize=15)\n",
        "plt.ylabel('Sepal width', fontsize=15)\n",
        "\n",
        "plt.xlim(xx.min(), xx.max())\n",
        "plt.ylim(yy.min(), yy.max())\n",
        "plt.xticks(())\n",
        "plt.yticks(())\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHEHu01OJoZv"
      },
      "source": [
        "## References:\n",
        "* [Scikit Learn Library](https://scikit-learn.org/stable/supervised_learning.html#supervised-learning)\n",
        "* [Logistic Regression for Machine Learning by Jason Brownlee PhD](https://machinelearningmastery.com/logistic-regression-for-machine-learning/)\n",
        "* [Advertising - Logistic Regression](https://www.kaggle.com/arpitsomani/advertisement-logistic-regression#Predictions-and-Evaluations)"
      ]
    }
  ]
}