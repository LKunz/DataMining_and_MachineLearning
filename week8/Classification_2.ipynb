{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Classification_2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/michalis0/DataMining_and_MachineLearning/blob/master/week8/Classification_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwKWzkX4sUzG"
      },
      "source": [
        "# Data Mining and Machine Learning - Week 8\n",
        "# Classification 2\n",
        "\n",
        "Classification is a core technique in the fields of data science and machine learning that is used to predict the categories to which data should belong. There are many algorithms that we can use, for example:\n",
        "* Logistic Regression (already covered)\n",
        "* K-Nearest Neighbours (KNN)\n",
        "* Decision Tree\n",
        "* Random Forest\n",
        "* Support Vector Machines (SVM)\n",
        "* Naive Bayes\n",
        "* Neural Networks\n",
        "* Etc.\n",
        "\n",
        "Documentation about supervised learning algorithms (including classification and regression) with sklearn can be found [here](https://scikit-learn.org/stable/supervised_learning.html#supervised-learning). This week we cover **K-Nearest Neighbours (KNN)** and **Decision Tree**.\n",
        "\n",
        "## Table of Contents\n",
        "#### 1. Recap on some important concepts\n",
        "* 1.1 KNN\n",
        "* 1.2 Decision Tree\n",
        "\n",
        "#### 2. Basic examples\n",
        "* 2.1 KNN\n",
        "* 2.2 Decision Tree\n",
        "* 2.3 Exercise\n",
        "\n",
        "#### 3. Drug classification with different algorithms\n",
        "* 3.1 Load, Clean and Explore Data\n",
        "* 3.2 Prepare data for algorthms\n",
        "* 3.3 KNN\n",
        "* 3.4 Decision Tree\n",
        "\n",
        "Author: Luc Kunz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Npu1v_aqCOO"
      },
      "source": [
        "## 1. Recap on some important concepts\n",
        "We first recap some concepts seen in class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oX9rJjEPP_S"
      },
      "source": [
        "### 1.1 KNN\n",
        "\n",
        "In the K-Nearest Neighbors algorithm, in order to classify a point, we measure the distance (e.g. Euclidean distance) to the nearest k instances of the training set, and let them vote. K is typically chosen to be an odd number.\n",
        "\n",
        "![KNN](https://miro.medium.com/max/1300/0*Sk18h9op6uK9EpT8.)\n",
        "\n",
        "\n",
        "The KNN algorithm is very useful when there are non-linear decision boundaries, as shown below.\n",
        "\n",
        "![KNN2](https://miro.medium.com/max/374/1*-W7HOfNfWk5BeXgF5jao6g.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "resaJFoDWA1w"
      },
      "source": [
        "### 1.2 Decision Tree\n",
        "\n",
        "In this case, we use a tree to classify new data points. The tree is built based on the training set. At each node, the algorithm chooses the split that maximizes a certain criterion (e.g. Gini index, information gain). The objective of the algorithm is to find the simplest possible tree (i.e. only a few nodes and a small depth) with high accuracy. Like KNN, the decision tree method is very useful when there are non-linear decision boundaries.\n",
        "\n",
        "In the example below, if we would have chosen another criterion for the first split (e.g. \"Exercises in the morning\"), we could have ended up with a lower accuracy and/or more splits (i.e. a more complex tree). \n",
        "\n",
        "![DT2](https://cdn.educba.com/academy/wp-content/uploads/2019/05/is-a-person-fit.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvO_eR7nqI0j"
      },
      "source": [
        "## 2. Basic examples\n",
        "We illustrate KNN and Decision Tree with simple data sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJ2nghivqQw3"
      },
      "source": [
        "# Import\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "\n",
        "# Customize plots\n",
        "%matplotlib inline\n",
        "sns.set_theme(style=\"white\")\n",
        "plt.style.use('dark_background')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_irgNanWGoL"
      },
      "source": [
        "### 2.1 KNN\n",
        "We create data from scratch. We generate 16 points in the plane [0,1]. Points with low values of x1 and x2 are associated with class 0 and points with high values of x1 and x2 are associated with class 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFaVbSpZqHVc"
      },
      "source": [
        "# Create Data\n",
        "data = {\"x1\":[0, 0.4, 0.15, 0.05, 0.4, 0.20, 0, 0.45, 1, 0.85, 0.9, 0.7, 0.65, 0.95, 1, 0.8],\n",
        "\"x2\":[0.2, 0.35, 0, 0.10, 0.4, 0.25, 0.40, 0.35, 0.85, 0.95, 1, 0.65, 0.75, 0.9, 0.9, 0.95],\n",
        "\"y\":[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
        "\n",
        "data = pd.DataFrame(data)\n",
        "data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "at3Q8Tvuqhbb"
      },
      "source": [
        "We also have 3 points for which we do not know the class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPzW2PLUw7PE"
      },
      "source": [
        "# New points\n",
        "p = pd.DataFrame({\"name\":[\"p1\", \"p2\", \"p3\"], \"x1\":[0.15, 0.75, 0.5],\n",
        "\"x2\":[0.35, 0.8, 0.6]})\n",
        "p"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2GaCblLu0FH"
      },
      "source": [
        "# Plot\n",
        "data.plot.scatter(\"x1\", \"x2\", c=\"y\", colormap=\"coolwarm_r\", figsize=(15, 10))\n",
        "plt.scatter(p.x1, p.x2, c=\"white\", marker=\"x\")\n",
        "for point in p.values:\n",
        "  plt.text(point[1]+0.01, point[2], point[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJuADOPoq2LI"
      },
      "source": [
        "The two classes can be identified on the above scatter plot. In addition, p1 seems to belong to class 0, p2 to class 1, and this is more difficult for p3. Below we classify the new points using the KNN algorithm with different k (i.e. the number of neightboor we consider)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Ly5UMD_x8Rb"
      },
      "source": [
        "# Select X and y\n",
        "X = data[[\"x1\", \"x2\"]]\n",
        "y = data[\"y\"]\n",
        "\n",
        "# KNN plot\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "fig, ax = plt.subplots(1, 3, figsize=(20,5))\n",
        "i = 0\n",
        "for k in [1, 3, 5]:\n",
        "  model = KNeighborsClassifier(n_neighbors=k).fit(X,y)\n",
        "  pred = model.predict(p[[\"x1\", \"x2\"]])\n",
        "  ax[i].scatter(data.x1, data.x2, c=data.y, cmap=\"coolwarm_r\")\n",
        "  ax[i].scatter(p.x1, p.x2, c=pred, cmap=\"coolwarm_r\", marker=\"x\")\n",
        "  ax[i].set_title(\"KNN with k = \" + str(k))\n",
        "  i += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jB0jxPytQPj"
      },
      "source": [
        "For k = 1 and k = 3, p3 belongs to class 0 while it belongs to class 1 for k = 5."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWH8CXE9WKFJ"
      },
      "source": [
        "### 2.2 Decision Tree\n",
        "We now present the concept of the decision tree using, again, basic examples. We predict the salary class (0 or 1) according to individual characteristics. With the sklearn, the criterion for determining the split at each node is the [Gini index](https://medium.com/analytics-steps/understanding-the-gini-index-and-information-gain-in-decision-trees-ab4720518ba8)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cgwHKHAZZcb"
      },
      "source": [
        "# Change style (to make the trees below beautiful)\n",
        "plt.style.use('classic')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMod-jACvqm9"
      },
      "source": [
        "# Decision tree\n",
        "example = 1 # 1, 2 or 3 \n",
        "\n",
        "if example == 1:\n",
        "  data = {\"Degree\":[\"Apprenticeship\", \"Apprenticeship\", \"Master\", \"Bachelor\", \"Master\", \"Apprenticeship\", \"Bachelor\", \"Bachelor\", \"Master\", \"Master\"],\n",
        "  \"Sex\":[1, 1, 0, 1, 0, 1, 1, 0, 1, 1], \n",
        "  \"Salary Class\":[0, 0, 1, 0, 1, 0, 0, 1, 1, 1]}\n",
        "  data = pd.DataFrame(data)\n",
        "elif example == 2:\n",
        "  data = {\"Age\":[20, 16, 50, 23, 36, 33, 41, 22, 27, 57],\n",
        "  \"Degree\":[\"Apprenticeship\", \"School\", \"Master\", \"Bachelor\", \"Bachelor\", \"Apprenticeship\", \"Bachelor\", \"Bachelor\", \"Master\", \"Master\"],\n",
        "  \"Sex\":[0, 1, 0, 1, 0, 1, 0, 1, 0, 1], \n",
        "  \"Salary Class\":[0, 0, 0, 0, 1, 1, 0, 0, 1, 1]}\n",
        "  data = pd.DataFrame(data)\n",
        "elif example == 3:\n",
        "  data = {\"Degree\":[\"Apprenticeship\", \"School\", \"Master\", \"Bachelor\", \"Bachelor\", \"Apprenticeship\", \"Bachelor\", \"Bachelor\", \"Master\", \"Master\"],\n",
        "  \"Sex\":[0, 1, 0, 1, 0, 1, 0, 1, 0, 1], \n",
        "  \"Salary Class\":[1, 1, 0, 1, 1, 0, 0, 0, 1, 1]}\n",
        "  data = pd.DataFrame(data)\n",
        "else:\n",
        "  data = np.nan\n",
        "  raise ValueError(\"'example' should be 1, 2 or 3\")\n",
        "\n",
        "data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSfLqzY-KHWo"
      },
      "source": [
        "We have three examples.\n",
        "\n",
        "Example 1: \n",
        "* All \"Master\" belong to class 1.\n",
        "* Among the rest, if sex == 0, then class 1.\n",
        "* A human could do the classification (easy).\n",
        "\n",
        "Example 2:\n",
        "* More difficult to see something...\n",
        "* Hint: look at young people...\n",
        "\n",
        "Example 3:\n",
        "* Illustrates that it is sometimes difficut to classify.\n",
        "* This is because of lack of pattern in the data.\n",
        "* If there is nothing to discover, then the algorithm will discover nothing...\n",
        "* The tree below illustrates this well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdVx7-8AR3bs"
      },
      "source": [
        "# Encode the categorical feature\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "one_hot = OneHotEncoder()\n",
        "one_hot_degree = one_hot.fit_transform(data[[\"Degree\"]]).toarray()\n",
        "one_hot_degree = pd.DataFrame(one_hot_degree, columns=one_hot.get_feature_names())\n",
        "one_hot_degree"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vDIaknpTGFv"
      },
      "source": [
        "# Concat\n",
        "data_tree = pd.concat([data, one_hot_degree], axis=1)\n",
        "data_tree"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsvhV78Wvq5h"
      },
      "source": [
        "# Select X and y\n",
        "X = data_tree.drop([\"Degree\", \"Salary Class\"], axis=1)\n",
        "y = data_tree[\"Salary Class\"]\n",
        "\n",
        "# Classification\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "tree = DecisionTreeClassifier()\n",
        "tree.fit(X, y)\n",
        "tree.score(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZcmu5EWTeuK"
      },
      "source": [
        "plt.figure(figsize=(7,7))\n",
        "plot_tree(tree, filled=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTERI0jL9xQT"
      },
      "source": [
        "# To see the splits\n",
        "pd.concat([X, y], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyaYoFnUniu9"
      },
      "source": [
        "#### **Important note**: if you end up with a deep tree and an **train** accuracy close to or equal to 1, there is a big probability of overfitting, as explained [here](https://towardsdatascience.com/how-to-find-decision-tree-depth-via-cross-validation-2bf143f0f3d6). The solution to avoid overfitting and find the best hyperparameters is to use cross-validation. More on this in Section 3.3.\n",
        "\n",
        "Question for the students: what is a deep tree?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ogo9OyixRcEP"
      },
      "source": [
        "### 2.3 Exercise\n",
        "To do in groups: follow the steps and send your answers and code @Luc Kunz on Slack (direct message) or via Zoom (private). This is a good way to improve your participation grade.\n",
        "\n",
        "The objective is to illustrate the power of decision trees with data having non-linear boundaries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQmLmLYfSNJw"
      },
      "source": [
        "# Data Set\n",
        "fruits = {\"width\":[3, 3.5, 3.5, 2.5, 4, 3.2, 3.6, 4.0, 2.8, 3.9, 7.7, 7.2, 7.8, 8.3, 7.3, 7.1, 8.5, 7.3, 9.2, 7.9, 7.3, 8.7],\n",
        "\"height\":[1.5, 2.5, 2, 1.3, 2.1, 7.4, 8.3, 7.9, 9.1, 8.5, 8.1, 7.8, 6.9, 7.4, 7.1, 7.1, 3.8, 4.2, 4.9, 5.4, 3.8, 4.4],\n",
        "\"fruit\":[\"orange\", \"orange\", \"orange\", \"orange\", \"orange\", \"apple\", \"apple\", \"apple\", \"apple\", \"apple\", \"orange\", \"orange\", \"orange\", \"orange\", \"orange\", \"orange\", \"apple\", \"apple\", \"apple\", \"apple\", \"apple\", \"apple\"]}\n",
        "\n",
        "fruits = pd.DataFrame(fruits)\n",
        "fruits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DKx8aTJiPd6"
      },
      "source": [
        "# Plot\n",
        "sns.swarmplot(fruits.width, fruits.height, hue=fruits.fruit)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bzB4GyHmSIu"
      },
      "source": [
        "# Select variables\n",
        "X = fruits[[\"width\", \"height\"]]\n",
        "y = fruits.fruit"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCubtGAEmLIz"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "# 1. Apply a Logistic Regression to the data. What is the train accuracy?\n",
        "LR = LogisticRegression()\n",
        "LR.fit(X, y)\n",
        "print(LR.score(X, y)) # 0.545454545\n",
        "# Explain in one sentence why it is not an appropriate algorithm.\n",
        "# --> Because the boundaries are non linear.\n",
        "\n",
        "# 2. Apply a Decision Tree to the data. Plot the tree. What is the depth of the tree? What is the train accuracy?\n",
        "tree = DecisionTreeClassifier()\n",
        "tree.fit(X, y)\n",
        "print(tree.score(X, y)) # 1.0\n",
        "print(tree.get_depth()) # 3\n",
        "plot_tree(tree, filled=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OScpbfpNV0FG"
      },
      "source": [
        "## 3. Drug Classification\n",
        "We classify people into drug categories according to their individual characteristics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faHiUxP6ZcWx"
      },
      "source": [
        "### 3.1 Load, Clean and Explore Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjbMSebFfy3F"
      },
      "source": [
        "# Load dataset\n",
        "url = \"https://raw.githubusercontent.com/michalis0/DataMining_and_MachineLearning/master/week8/data/drug200.csv\"\n",
        "df = pd.read_csv(url)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bd89fWxOP6aq"
      },
      "source": [
        "The variables:\n",
        "\n",
        "* Age: Age of patient\n",
        "* Sex: Gender of patient\n",
        "* BP: Blood pressure of patient\n",
        "* Cholesterol: Cholesterol of patient\n",
        "* Na_to_K: Sodium to Potassium Ratio in Blood\n",
        "* Drug: Drug Type"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2otdRInZNcT"
      },
      "source": [
        "df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NcVceQKJZSmI"
      },
      "source": [
        "df.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFm6BDY7Z4hQ"
      },
      "source": [
        "# Univariate Analysis\n",
        "fig, ax = plt.subplots(3, 2, figsize=(20,15))\n",
        "i = 0\n",
        "j = 0\n",
        "for var in df:\n",
        "  if df[var].dtypes == \"object\":\n",
        "    sns.countplot(x=df[var], ax=ax[i, j])\n",
        "  else:\n",
        "    sns.histplot(df[var], ax=ax[i, j])\n",
        "  i += 1\n",
        "  if i == 3:\n",
        "    i = 0\n",
        "    j += 1\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wj-INS2_QWP4"
      },
      "source": [
        "# What is the base rate?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJEvXYYTjUmp"
      },
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "sns.pairplot(df, hue=\"Drug\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9CMFLdbyQw-K"
      },
      "source": [
        "We see that all people with a Na_to_K ration above around 15 take DrugY. This will be useful for classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZmQvvtdj8uX"
      },
      "source": [
        "### 3.2 Prepare data for algorthms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6-ulxgXir5l"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxGYBM4Mh-6N"
      },
      "source": [
        "# Label Encoding\n",
        "df.Sex = df.Sex.astype('category').cat.codes\n",
        "df.BP = df.BP.astype('category').cat.codes\n",
        "df.Cholesterol = df.Cholesterol.astype('category').cat.codes\n",
        "df.Drug = df.Drug.astype('category').cat.codes\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7JuXQRykCW4"
      },
      "source": [
        "# Select Features\n",
        "X = df.drop([\"Drug\"], axis=1)\n",
        "y = df.Drug\n",
        "\n",
        "# Train test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.35, random_state=72)\n",
        "X_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOPuiBqekmOG"
      },
      "source": [
        "y_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJJWrJoelD9c"
      },
      "source": [
        "### 3.3 KNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQxYdNAn_24P"
      },
      "source": [
        "We first need to normalize (i.e. rescale) the data since we are using distances. The `MinMaxScaler()` maps the data in the interval (0,1)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7uyNJdmHkmR_"
      },
      "source": [
        "# Normalization (since distance)\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(X_train) \n",
        "X_train_norm = scaler.transform(X_train)\n",
        "X_test_norm = scaler.transform(X_test)\n",
        "X_train_norm[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z224TwfGkmVU"
      },
      "source": [
        "# Fit KNN model\n",
        "k = 5\n",
        "knn = KNeighborsClassifier(n_neighbors=k)\n",
        "knn.fit(X_train_norm, y_train)\n",
        "\n",
        "# Prediction for test set\n",
        "y_pred = knn.predict(X_test_norm)\n",
        "\n",
        "# Evaluate model\n",
        "def accuracy_conf_mat(y_test, y_pred):\n",
        "  print(round(accuracy_score(y_test, y_pred), 4))\n",
        "  conf_mat = confusion_matrix(y_test, y_pred)\n",
        "  fig, ax = plt.subplots(figsize=(10,10))\n",
        "  sns.heatmap(conf_mat, annot=True, fmt='d')\n",
        "  plt.ylabel('Actual')\n",
        "  plt.xlabel('Predicted')\n",
        "  plt.show()\n",
        "\n",
        "accuracy_conf_mat(y_test, y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hv3z3x4HAxgH"
      },
      "source": [
        "We now want to test which hyperparameters of the [KNN class](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) are the optimal ones. For this we use [Grid Search Cross Validation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQjrI_9tkmGF"
      },
      "source": [
        "# Grid Search - hyperparameters tunning\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define parameters to test\n",
        "grid = {'n_neighbors':np.arange(1,100),\n",
        "        'p':np.arange(1,3),\n",
        "        'weights':['uniform','distance']\n",
        "       }\n",
        "\n",
        "# Define and fit model\n",
        "knn = KNeighborsClassifier()\n",
        "knn_cv = GridSearchCV(knn, grid, cv=10)\n",
        "knn_cv.fit(X_train_norm, y_train)\n",
        "\n",
        "# Print results\n",
        "print(\"Hyperparameters:\", knn_cv.best_params_)\n",
        "print(\"Train Score:\", round(knn_cv.best_score_, 4))\n",
        "print(\"Test Score:\", round(knn_cv.score(X_test_norm, y_test), 4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1dDiXjZJ3lo"
      },
      "source": [
        "# Fit optimal KNN model\n",
        "knn = KNeighborsClassifier(n_neighbors=1, p=2, weights='uniform')\n",
        "knn.fit(X_train_norm, y_train)\n",
        "\n",
        "# Prediction for test set\n",
        "y_pred = knn.predict(X_test_norm)\n",
        "\n",
        "# Evaluate model\n",
        "def accuracy_conf_mat(y_test, y_pred):\n",
        "  print(round(accuracy_score(y_test, y_pred), 4))\n",
        "  conf_mat = confusion_matrix(y_test, y_pred)\n",
        "  fig, ax = plt.subplots(figsize=(10,10))\n",
        "  sns.heatmap(conf_mat, annot=True, fmt='d')\n",
        "  plt.ylabel('Actual')\n",
        "  plt.xlabel('Predicted')\n",
        "  plt.show()\n",
        "\n",
        "accuracy_conf_mat(y_test, y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Dvixd3WjyZ_"
      },
      "source": [
        "### 3.4 Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qpr0ZIYMoHYN"
      },
      "source": [
        "# Fit model\n",
        "tree = DecisionTreeClassifier()\n",
        "tree.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = tree.predict(X_test)\n",
        "\n",
        "# Evaluate model\n",
        "accuracy_conf_mat(y_test, y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXZilnquoH5P"
      },
      "source": [
        "plt.figure(figsize=(12, 12))\n",
        "plot_tree(tree, filled=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYW6rQD3rXL0"
      },
      "source": [
        "# First split\n",
        "training_data = pd.concat([X_train, y_train], axis=1)\n",
        "training_data[training_data.Na_to_K > 14.627]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jqi-T_RYssjN"
      },
      "source": [
        "# Accuracy in training set\n",
        "tree.score(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPLKdgNMFCqY"
      },
      "source": [
        "We are not in the above-mentioned case of overfitting here since good results in test data. In addition a depth of 4 with 5 classes is optimal. With real life data, this may be different... Grid search cross-validation is not useful in this case. We can however try with different depths."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3N55g4nFzUN"
      },
      "source": [
        "# Look for simpler trees\n",
        "for depth in [1, 2, 3, 4]:\n",
        "    tree = DecisionTreeClassifier(max_depth=depth).fit(X_train, y_train)\n",
        "    y_pred = tree.predict(X_test)\n",
        "    print(\"Depth: \" + str(depth))\n",
        "    print(round(accuracy_score(y_test, y_pred), 2))\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    plot_tree(tree, filled=True)\n",
        "    plt.show()\n",
        "    print(\"\\n\\n\\n\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1_PtfIztRMQ"
      },
      "source": [
        "## References:\n",
        "https://www.kaggle.com/gorkemgunay/drug-classification-with-different-algorithms/notebook"
      ]
    }
  ]
}